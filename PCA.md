## 降维

## 要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。

只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。实际上任何两个线性无关的二维向量都可以成为一组基。

所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。

最后，上述分析同时给矩阵相乘找到了一种物理解释：
两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。
更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。

## 协方差矩阵及优化目标

上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。
但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。
或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？

拿一组二维数据举例子，先将它们均值化后，进行降维处理也就是将它们映射到一条直线上。
那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。

## 而这种分散程度，可以用数学上的方差来表述，即：

$$Var(a) = \frac{1}{m}\sum_{i=1}^{m}(a_i - \mu)^2 $$

于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，**方差值最大**。

对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。

## 但当从二维变为多维时应该怎么办

如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。
数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：

$$Cov(a,b) = \frac{1}{m}\sum_{1}^{m}a_ib_i $$


